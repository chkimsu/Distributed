{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from larva import LarvaTokenizer, LarvaModel\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, models, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from sentence_transformers.pair_data import PairData\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def train(gpu, ngpus_per_node, args):\n",
    "    \n",
    "    args.gpu = gpu + args.start_gpu_num\n",
    "    device = torch.device(\"cuda:{}\".format(args.gpu))\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        logging.debug(\"Use GPU: {} for training\".format(args.gpu))\n",
    "\n",
    "    args.rank = args.rank * ngpus_per_node + gpu\n",
    "    print(\"gpu :\", args.gpu, \"rank :\", args.rank, \"world size\", args.world_size)\n",
    "    dist.init_process_group(\"nccl\", init_method='tcp://127.0.0.1:39501',\n",
    "                            rank=args.rank, world_size=args.world_size)   \n",
    "    torch.cuda.set_device(args.gpu)\n",
    "\n",
    "    # 1. For word embedding and pooling\n",
    "    model = LarvaModel.from_pretrained(args.larva_model)\n",
    "    tokenizer = LarvaTokenizer.from_pretrained(args.larva_model)\n",
    "    model.save_pretrained(args.root_dir + args.pt_model_dir)\n",
    "    tokenizer.save_pretrained(args.root_dir + args.pt_model_dir)\n",
    "\n",
    "    # 2. path\n",
    "    train_data_path = args.data_dir + \"/\" + args.train_dir + \"/\"\n",
    "    valid_data_path = args.data_dir + '/valid/'\n",
    "    test_data_path = args.data_dir + '/test/'\n",
    "\n",
    "    # 3. pre trained larva model\n",
    "    word_embedding_model = models.Transformer(args.root_dir + args.pt_model_dir) # n*3*256\n",
    "    cnn = models.CNN(\n",
    "        in_word_embedding_dimension=word_embedding_model.get_word_embedding_dimension(), \n",
    "        out_channels=256, \n",
    "        kernel_sizes=[1,3,5]\n",
    "    ) # n*3*768 (256*3)\n",
    "    pooling_model = models.Pooling(\n",
    "        cnn.get_word_embedding_dimension(), \n",
    "    pooling_mode_mean_tokens=True, \n",
    "        pooling_mode_cls_token=False, \n",
    "        pooling_mode_max_tokens=False\n",
    "    )\n",
    "    dropout = models.Dropout(0.2)\n",
    "    sent_embeddings_dimension = pooling_model.get_sentence_embedding_dimension()\n",
    "    dan1 = models.Dense(in_features=sent_embeddings_dimension, out_features=sent_embeddings_dimension,)\n",
    "    dan2 = models.Dense(in_features=sent_embeddings_dimension, out_features=args.output_dim)\n",
    "\n",
    "    # sbert_model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dan1, dan2], device=device)\n",
    "    sbert_model = SentenceTransformer(modules=[word_embedding_model, cnn, pooling_model, dan1, dropout, dan2], device=device)\n",
    "\n",
    "    # 4. data loader\n",
    "    # valid_pair = PairData(valid_data_path)\n",
    "    # valid_samples = valid_pair.get_example(shuffle=False, num_data=args.valid_size, gpu=args.gpu)\n",
    "    test_pair = PairData(test_data_path )\n",
    "    test_samples = test_pair.get_example(shuffle=False, num_data=args.valid_size, gpu=args.gpu)\n",
    "    train_pair = PairData(train_data_path)\n",
    "    train_data_iter = train_pair.get_data_iter(args.batch_size, is_train=True, duplicates=args.duplicates, gpu=args.gpu)\n",
    "\n",
    "    # 5. loss function\n",
    "    evaluator = BinaryClassificationEvaluator.from_input_examples(test_samples, name=args.data_name)\n",
    "    evaluation_steps = int(args.train_size/args.batch_size)\n",
    "    if args.loss_type == \"contrastive\":\n",
    "        train_loss = losses.ContrastiveLoss(model=sbert_model)\n",
    "    elif args.loss_type == \"online_contrastive\":\n",
    "        train_loss = losses.OnlineContrastiveLoss(model=sbert_model)\n",
    "    elif args.loss_type == \"cos_sim\":\n",
    "        train_loss = losses.CosineSimilarityLoss(model=sbert_model)\n",
    "    elif args.loss_type == \"on_cont_cross_ent_mt\":\n",
    "        train_loss_1 = losses.OnlineContrastiveLoss(model=sbert_model)\n",
    "        train_loss_2 = losses.SoftmaxLoss(model=sbert_model, num_labels=2, sentence_embedding_dimension=pooling_model.get_sentence_embedding_dimension())\n",
    "    elif args.loss_type == \"on_cont_cross_ent\":\n",
    "        train_loss = losses.OnlineContrastiveCrossEntropyLoss(\n",
    "            model=sbert_model,\n",
    "            num_labels=2, \n",
    "            sentence_embedding_dimension=pooling_model.get_sentence_embedding_dimension()\n",
    "            )\n",
    "    warmup_steps = math.ceil(args.train_size * args.epochs * args.warmup_rate)\n",
    "\n",
    "    #5. model 이름\n",
    "    model_output_name = \"\".join(args.ymd.split(\"-\")) + \"_{}_{}_{}e_{}bs_{}_{}\" \\\n",
    "            .format(args.pt_model_dir.split(\"/\")[-1], args.loss_type, args.epochs, args.batch_size, args.train_dir, args.output_dim)\n",
    "\n",
    "    # 6. tensorboard\n",
    "    writer = SummaryWriter(args.tensorboard_path + \"/\" + model_output_name)\n",
    "\n",
    "    # 7. train\n",
    "    output_model_dir = args.output_model_dir + '/' + model_output_name \n",
    "    sbert_model.fit(\n",
    "        train_objectives=[(train_data_iter, train_loss)],\n",
    "        # train_objectives=[(train_data_iter, train_loss_1), (train_data_iter, train_loss_2)],\n",
    "        evaluator=evaluator,\n",
    "        epochs=args.epochs,\n",
    "        evaluation_steps=evaluation_steps,\n",
    "        warmup_steps=warmup_steps,\n",
    "        output_path=args.root_dir + output_model_dir,\n",
    "        show_progress_bar=False,\n",
    "        writer=writer, \n",
    "        device_ids=[args.gpu], \n",
    "        output_device=args.gpu,\n",
    "        rank=args.rank,\n",
    "        # optimizer_params={'lr': 2e-5}\n",
    "        optimizer_params={'lr': 0.0003}\n",
    "        # optimizer_params={'lr': 0.0005}\n",
    "    )\n",
    "\n",
    "\n",
    "def test(args):\n",
    "    model_output_name = \"\".join(args.ymd.split(\"-\")) + \"_{}_{}_{}e_{}bs_{}_{}\" \\\n",
    "            .format(args.pt_model_dir.split(\"/\")[-1], args.loss_type, args.epochs, args.batch_size, args.train_dir, args.output_dim)\n",
    "    output_model_dir = args.output_model_dir + \"/\" + model_output_name\n",
    "    sbert_model = SentenceTransformer(\n",
    "        args.root_dir + output_model_dir, \n",
    "        device=\"cuda:{}\".format(args.gpu)\n",
    "        )\n",
    "    \n",
    "\n",
    "    test_data_path = args.data_dir \n",
    "\n",
    "    test_pair = PairData(test_data_path)\n",
    "    test_samples = test_pair.get_example(shuffle=False, num_data=args.valid_size, gpu=args.gpu)\n",
    "\n",
    "    test_evaluator = BinaryClassificationEvaluator.from_input_examples(test_samples, name=args.data_name)\n",
    "    test_evaluator(sbert_model)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    if args.is_test:\n",
    "        test(args)\n",
    "    else:\n",
    "        # train(args)\n",
    "        ngpus_per_node = args.ngpus \n",
    "        args.world_size = ngpus_per_node * 1\n",
    "        mp.spawn(train,\n",
    "            args=(ngpus_per_node, args),\n",
    "            nprocs=ngpus_per_node,\n",
    "            join=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='BM training')\n",
    "    parser.add_argument('--root-dir', type=str)\n",
    "    parser.add_argument('--data-dir', type=str)\n",
    "    parser.add_argument('--pt-model-dir', type=str)\n",
    "    parser.add_argument('--output-model-dir', type=str)\n",
    "    parser.add_argument('--larva-model', type=str)\n",
    "    parser.add_argument('--task', type=str)\n",
    "    parser.add_argument('--batch-size', type=int)\n",
    "    parser.add_argument('--train-size', type=int)\n",
    "    parser.add_argument('--valid-size', type=int)\n",
    "    parser.add_argument('--data-name', type=str)\n",
    "    parser.add_argument('--epochs', type=int)\n",
    "    parser.add_argument('--warmup-rate', type=float)\n",
    "    parser.add_argument('--gpu', type=int)\n",
    "    parser.add_argument('--test-type', type=str)\n",
    "    parser.add_argument('--is-test', default=False, action='store_true')\n",
    "    parser.add_argument('--test-size', type=int)\n",
    "    parser.add_argument('--ymd', type=str)\n",
    "    parser.add_argument('--loss-type', type=str)\n",
    "    parser.add_argument('--tensorboard-path', type=str)\n",
    "    parser.add_argument('--train-dir', type=str)\n",
    "    parser.add_argument('--ngpus', type=int)\n",
    "    parser.add_argument('--start-gpu-num', type=int)\n",
    "    parser.add_argument('--rank', default=0, type=int)\n",
    "    parser.add_argument('--duplicates', default=1, type=int)\n",
    "    parser.add_argument('--output-dim', type=int)\n",
    "\n",
    "    # parser.add_argument('--rank', default=0, type=int)\n",
    "    # parser.add_argument('--world-size', default=1, type=int)\n",
    "    # parser.add_argument('--ngpus', '--ngpus-per-node-size', default=2, type=int)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_row', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.preprocessing import sequence, text\n",
    "from xgboost import XGBClassifier \n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Data :  19579\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EAP</td>\n",
       "      <td>7900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MWS</td>\n",
       "      <td>6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HPL</td>\n",
       "      <td>5635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index  author\n",
       "0   EAP    7900\n",
       "1   MWS    6044\n",
       "2   HPL    5635"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('./spooky_data/train.csv')\n",
    "print('Length of Data : ', len(df))\n",
    "display(df.head())\n",
    "display(df.author.value_counts().reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***TEXT Pattern***\n",
    "- 파생변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>avg_word_len_not_stopword</th>\n",
       "      <th>num_commas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>231</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.658537</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.583333</td>\n",
       "      <td>5.952381</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>206</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.088235</td>\n",
       "      <td>6.304348</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.481481</td>\n",
       "      <td>7.437500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         41                35        231             19                 7   \n",
       "1         14                14         71              8                 1   \n",
       "2         36                32        200             16                 5   \n",
       "3         34                32        206             13                 4   \n",
       "4         27                25        174             11                 4   \n",
       "\n",
       "   num_words_upper  num_words_title  avg_word_len  avg_word_len_not_stopword  \\\n",
       "0                2                3      4.658537                   6.000000   \n",
       "1                0                1      4.142857                   5.714286   \n",
       "2                0                1      4.583333                   5.952381   \n",
       "3                0                4      5.088235                   6.304348   \n",
       "4                0                2      5.481481                   7.437500   \n",
       "\n",
       "   num_commas  \n",
       "0           4  \n",
       "1           0  \n",
       "2           4  \n",
       "3           3  \n",
       "4           2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pattern_extract(df, stopwords=stop_words):\n",
    "    \n",
    "    df_re = df.copy()\n",
    "    \n",
    "    ## Number of words in the text : 단어 수 ##\n",
    "    df_re[\"num_words\"] = df_re[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    ## Number of unique words in the text : 유니크 단어 수 ##\n",
    "    df_re[\"num_unique_words\"] = df_re[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    ## Number of characters in the text : 문자 수 ##\n",
    "    df_re[\"num_chars\"] = df_re[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "    ## Number of stopwords in the text : 불용어 수 ##\n",
    "    df_re[\"num_stopwords\"] = df_re[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n",
    "\n",
    "    ## Number of punctuations in the text : 특수문자 수 ##\n",
    "    df_re[\"num_punctuations\"] =df_re['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "    ## Number of title case words in the text : 대문자 수 ##\n",
    "    df_re[\"num_words_upper\"] = df_re[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "    ## Number of title case words in the text : 단어의 첫글자가 대문자인 단어 수 ##\n",
    "    df_re[\"num_words_title\"] = df_re[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "    ## Average length of the words in the text : 평균 문자 수 ##\n",
    "    df_re[\"avg_word_len\"] = df_re[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "    \n",
    "    ## Average length of the words(except stop words) in the text : 불용어가 아닌 단어의 평균 문자 수 ##\n",
    "    df_re['avg_word_len_not_stopword'] = df_re['text'].apply(lambda x: np.mean([len(t) for t in x.split(' ') if t not in stopwords]))\n",
    "    \n",
    "    ## Number of comma\n",
    "    df_re['num_commas'] = df_re['text'].apply(lambda x: x.count(','))\n",
    "    \n",
    "    return df_re\n",
    "\n",
    "df_re = pattern_extract(df)\n",
    "display(df_re.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">num_words</th>\n",
       "      <th colspan=\"3\" halign=\"left\">num_unique_words</th>\n",
       "      <th colspan=\"3\" halign=\"left\">num_chars</th>\n",
       "      <th colspan=\"3\" halign=\"left\">num_stopwords</th>\n",
       "      <th colspan=\"3\" halign=\"left\">num_punctuations</th>\n",
       "      <th colspan=\"3\" halign=\"left\">num_words_upper</th>\n",
       "      <th colspan=\"3\" halign=\"left\">num_words_title</th>\n",
       "      <th colspan=\"3\" halign=\"left\">avg_word_len</th>\n",
       "      <th colspan=\"3\" halign=\"left\">avg_word_len_not_stopword</th>\n",
       "      <th colspan=\"3\" halign=\"left\">num_commas</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EAP</th>\n",
       "      <td>25.442405</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.567706</td>\n",
       "      <td>21.894937</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.727397</td>\n",
       "      <td>142.225949</td>\n",
       "      <td>115.0</td>\n",
       "      <td>105.751334</td>\n",
       "      <td>12.626456</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.546129</td>\n",
       "      <td>4.096329</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.573788</td>\n",
       "      <td>0.553291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.892966</td>\n",
       "      <td>2.102405</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.052241</td>\n",
       "      <td>4.644952</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.631340</td>\n",
       "      <td>6.033637</td>\n",
       "      <td>6.036376</td>\n",
       "      <td>0.944184</td>\n",
       "      <td>2.227089</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.445522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HPL</th>\n",
       "      <td>27.799645</td>\n",
       "      <td>26.0</td>\n",
       "      <td>14.123252</td>\n",
       "      <td>24.437977</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.053739</td>\n",
       "      <td>155.843478</td>\n",
       "      <td>142.0</td>\n",
       "      <td>82.020647</td>\n",
       "      <td>12.970186</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.853415</td>\n",
       "      <td>3.206921</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.108637</td>\n",
       "      <td>0.500266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.852313</td>\n",
       "      <td>2.334694</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.041579</td>\n",
       "      <td>4.625193</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.554917</td>\n",
       "      <td>5.889127</td>\n",
       "      <td>5.909091</td>\n",
       "      <td>0.873821</td>\n",
       "      <td>1.522804</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.345381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MWS</th>\n",
       "      <td>27.417273</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.134440</td>\n",
       "      <td>23.544672</td>\n",
       "      <td>21.0</td>\n",
       "      <td>14.925835</td>\n",
       "      <td>151.659828</td>\n",
       "      <td>130.0</td>\n",
       "      <td>126.305008</td>\n",
       "      <td>13.742224</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.080172</td>\n",
       "      <td>3.833719</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.840625</td>\n",
       "      <td>0.751489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.203636</td>\n",
       "      <td>2.124255</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.759572</td>\n",
       "      <td>4.598182</td>\n",
       "      <td>4.560791</td>\n",
       "      <td>0.561558</td>\n",
       "      <td>5.947698</td>\n",
       "      <td>5.947368</td>\n",
       "      <td>0.860704</td>\n",
       "      <td>1.992886</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.100672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        num_words                   num_unique_words                    \\\n",
       "             mean median        std             mean median        std   \n",
       "author                                                                   \n",
       "EAP     25.442405   21.0  18.567706        21.894937   19.0  13.727397   \n",
       "HPL     27.799645   26.0  14.123252        24.437977   23.0  11.053739   \n",
       "MWS     27.417273   23.0  23.134440        23.544672   21.0  14.925835   \n",
       "\n",
       "         num_chars                    num_stopwords                    \\\n",
       "              mean median         std          mean median        std   \n",
       "author                                                                  \n",
       "EAP     142.225949  115.0  105.751334     12.626456   10.0   9.546129   \n",
       "HPL     155.843478  142.0   82.020647     12.970186   12.0   6.853415   \n",
       "MWS     151.659828  130.0  126.305008     13.742224   12.0  12.080172   \n",
       "\n",
       "       num_punctuations                  num_words_upper                   \\\n",
       "                   mean median       std            mean median       std   \n",
       "author                                                                      \n",
       "EAP            4.096329    3.0  3.573788        0.553291    0.0  0.892966   \n",
       "HPL            3.206921    3.0  2.108637        0.500266    0.0  0.852313   \n",
       "MWS            3.833719    3.0  2.840625        0.751489    0.0  1.203636   \n",
       "\n",
       "       num_words_title                  avg_word_len                      \\\n",
       "                  mean median       std         mean    median       std   \n",
       "author                                                                     \n",
       "EAP           2.102405    1.0  2.052241     4.644952  4.600000  0.631340   \n",
       "HPL           2.334694    2.0  2.041579     4.625193  4.600000  0.554917   \n",
       "MWS           2.124255    2.0  1.759572     4.598182  4.560791  0.561558   \n",
       "\n",
       "       avg_word_len_not_stopword                     num_commas         \\\n",
       "                            mean    median       std       mean median   \n",
       "author                                                                   \n",
       "EAP                     6.033637  6.036376  0.944184   2.227089    2.0   \n",
       "HPL                     5.889127  5.909091  0.873821   1.522804    1.0   \n",
       "MWS                     5.947698  5.947368  0.860704   1.992886    2.0   \n",
       "\n",
       "                  \n",
       "             std  \n",
       "author            \n",
       "EAP     2.445522  \n",
       "HPL     1.345381  \n",
       "MWS     2.100672  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_re.groupby('author').agg(['mean','median','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13705, 11) (13705, 1) (5874, 11) (5874, 1)\n",
      "ACCURACY SCORE :  0.5187265917602997\n",
      "LOG LOSS: 0.979 \n"
     ]
    }
   ],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota\n",
    "\n",
    "\n",
    "def XGBoostModel(xtrain, xtest, ytrain, ytest, opt=False):\n",
    "    if opt: \n",
    "        xgb = XGBClassifier()\n",
    "        param_grid = {\n",
    "                \"n_estimators\":[100,200,300],\n",
    "                \"max_depth\":[4, 10, 20]\n",
    "            }\n",
    "\n",
    "        xgb_tuned = GridSearchCV(estimator=xgb,\n",
    "                                param_grid=param_grid,\n",
    "                                cv=3, \n",
    "                                n_jobs=-1,\n",
    "                                verbose=0)\n",
    "\n",
    "        xgb_tuned.fit(X=xtrain,\n",
    "                    y=ytrain)\n",
    "        \n",
    "        print('XGB MODEL Best score : {:.2f}%'.format(xgb_tuned.best_score_*100))\n",
    "\n",
    "        xgb_model = xgb_tuned.best_estimator_\n",
    "        \n",
    "    else:\n",
    "        xgb_model = XGBClassifier()\n",
    "        xgb_model.fit(X=xtrain,\n",
    "                y=ytrain)\n",
    "        \n",
    "    ypred = xgb_model.predict(xtest)\n",
    "    ypred_p = xgb_model.predict_proba(xtest)\n",
    "    \n",
    "    print('ACCURACY SCORE : ',accuracy_score(ytest.values.reshape(-1), ypred))\n",
    "    print (\"LOG LOSS: %0.3f \" % multiclass_logloss(ytest.values.reshape(-1), ypred_p))\n",
    "    \n",
    "    return xgb_model\n",
    "\n",
    "\n",
    "# Sample Modeling\n",
    "df_re['target'] = LabelEncoder().fit_transform(df_re['author'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_re.drop(['id','author','target'], axis=1), df_re[['target']], test_size=0.3, random_state=123, shuffle=True)\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "xgb = XGBoostModel(x_train.drop('text', axis=1), x_test.drop('text', axis=1), y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  ***A Deep Dive Into Sklearn Pipelines***\n",
    "> https://www.kaggle.com/code/baghern/a-deep-dive-into-sklearn-pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Transformer to select a single column from the data frame to perform additional transformations on\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "    \n",
    "\n",
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer(stop_words='english'))\n",
    "            ])\n",
    "\n",
    "text.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***TEXT Preprocessing***\n",
    "\n",
    "> 텍스트 전처리\n",
    "1. Tokenization : word tokenization\n",
    "2. Cleaning : remove noise (stopword)\n",
    "3. Normalization : data integration (stemming, lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***TEXT Conversion***\n",
    "---\n",
    "1. TF-IDF\n",
    "2. SVD\n",
    "3. Word2Vec\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. TF-IDF\n",
    "    1) TF-IDF TEXT VECTOR\n",
    "    2) TF-IDF TEXT VECTOR + TEXT INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13705, 11) (13705, 1) (5874, 11) (5874, 1)\n",
      "(13705, 14604) (5874, 14604)\n",
      "ACCURACY SCORE :  0.672454885938032\n",
      "LOG LOSS: 0.774 \n"
     ]
    }
   ],
   "source": [
    "# 1. TF-IDF Parameter\n",
    "# 1-1 TF-IDF Modeling\n",
    "import re\n",
    "def text_processing(tab):\n",
    "    for i in range(tab.shape[0]):\n",
    "        # remove special VW symbols\n",
    "        text = tab[\"text\"].loc[i].strip().replace('|', '').replace(':', '').lower() \n",
    "        words = re.findall(\"\\w{3,}\", text) \n",
    "        new_text = \" \".join(words) \n",
    "        tab.loc[i, 'new_text'] = new_text\n",
    "    return tab\n",
    "\n",
    "def tfidf(min_df=3, max_features=None, ngram=3):\n",
    "    tfv = TfidfVectorizer(min_df=min_df, # 최소 빈도값 설정 : 3개 이상의 문서에 출현한 단어만 사용\n",
    "                        max_features=max_features, # unique 단어 수 제한\n",
    "                        strip_accents='unicode', # 문자 정규화 {'ascii', 'unicode', None}\n",
    "                        analyzer='word', # {'word' : 학습 단위 = 단어, 'char' : 학습 단위 = 글자}\n",
    "                        token_pattern=r'\\w{1,}',\n",
    "                        ngram_range=(1, ngram), # 단어 묶음 설정 -> if (1, 3) : 'go', 'go back', 'go back to'\n",
    "                        use_idf=True,\n",
    "                        smooth_idf=True,\n",
    "                        sublinear_tf=True, # TF값의 smoothing 여부 -> if smoothin True : TF -> 1 + ln(TF)\n",
    "                        stop_words = 'english') # 불용어 제거 {'english', list : 사용자 설정, None}\n",
    "    return tfv\n",
    "\n",
    "# df_re = text_processing(df_re)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_re.drop(['id','author','target','text'], axis=1), df_re[['target']], test_size=0.3, random_state=123, shuffle=True)\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "tfv = tfidf()\n",
    "tfv.fit(df_re.new_text.values)\n",
    "\n",
    "xtrain_tfv =  tfv.transform(x_train.new_text.values) \n",
    "xvalid_tfv = tfv.transform(x_test.new_text.values)\n",
    "print(xtrain_tfv.toarray().shape, xvalid_tfv.toarray().shape)\n",
    "\n",
    "xgb = XGBoostModel(xtrain_tfv.tocsc(), xvalid_tfv.tocsc(), y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13705, 100) (5874, 100)\n",
      "(13705, 110) (5874, 110)\n",
      "ACCURACY SCORE :  0.5888661899897855\n",
      "LOG LOSS: 0.881 \n"
     ]
    }
   ],
   "source": [
    "# # 1-2 TF-IDF + Feature Modeling\n",
    "# tfv = tfidf(min_df=5, max_features=100, ngram=2)\n",
    "# tfv.fit(df_re.text.values)\n",
    "\n",
    "# xtrain_tfv =  tfv.transform(x_train.text.values) \n",
    "# xvalid_tfv = tfv.transform(x_test.text.values)\n",
    "# print(xtrain_tfv.toarray().shape, xvalid_tfv.toarray().shape)\n",
    "\n",
    "# x_train_re = pd.concat([pd.DataFrame(xtrain_tfv.toarray()), x_train.drop('text', axis=1).reset_index(drop=True)], axis=1)\n",
    "# x_test_re = pd.concat([pd.DataFrame(xvalid_tfv.toarray()), x_test.drop('text', axis=1).reset_index(drop=True)], axis=1)\n",
    "# print(x_train_re.shape, x_test_re.shape)\n",
    "\n",
    "# xgb = XGBoostModel(x_train_re, x_test_re, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:57:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.09257\ttest-mlogloss:1.09271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hdy25\\anaconda3\\lib\\site-packages\\xgboost\\core.py:525: FutureWarning: Pass `evals` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttrain-mlogloss:0.95783\ttest-mlogloss:0.97700\n",
      "[200]\ttrain-mlogloss:0.92573\ttest-mlogloss:0.96237\n",
      "[300]\ttrain-mlogloss:0.90443\ttest-mlogloss:0.95772\n",
      "[400]\ttrain-mlogloss:0.88823\ttest-mlogloss:0.95611\n",
      "[500]\ttrain-mlogloss:0.87493\ttest-mlogloss:0.95614\n",
      "[522]\ttrain-mlogloss:0.87209\ttest-mlogloss:0.95637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hdy25\\anaconda3\\lib\\site-packages\\xgboost\\core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1-3 \n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes\n",
    "import xgboost as xgb\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, seed_val=0, child=1, colsample=0.3, verbose_eval=100):\n",
    "    param = {'objective':'multi:softprob',\n",
    "             'eta':0.1,\n",
    "             'max_depth':3,\n",
    "             'silent':1,\n",
    "             'num_class':3,\n",
    "             'eval_metric':'mlogloss',\n",
    "             'min_child_weight':child,\n",
    "             'subsample':0.8,\n",
    "             'colsample_bytree':colsample,\n",
    "             'seed':seed_val}\n",
    "\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=verbose_eval)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    # if test_X2 is not None:\n",
    "    #     xgtest2 = xgb.DMatrix(test_X2)\n",
    "    #     pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, model\n",
    "\n",
    "\n",
    "# test\n",
    "pred_test_y, model = runXGB(x_train.drop('new_text', axis=1), y_train['target'],\n",
    "                            x_test.drop('new_text', axis=1), y_test['target'], seed_val=0, child=1, colsample=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:58:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.09270\ttest-mlogloss:1.09272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hdy25\\anaconda3\\lib\\site-packages\\xgboost\\core.py:525: FutureWarning: Pass `evals` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttrain-mlogloss:0.78036\ttest-mlogloss:0.84007\n",
      "[600]\ttrain-mlogloss:0.67268\ttest-mlogloss:0.76910\n",
      "[900]\ttrain-mlogloss:0.60080\ttest-mlogloss:0.72761\n",
      "[1200]\ttrain-mlogloss:0.54690\ttest-mlogloss:0.70075\n",
      "[1500]\ttrain-mlogloss:0.50372\ttest-mlogloss:0.68127\n",
      "[1800]\ttrain-mlogloss:0.46784\ttest-mlogloss:0.66751\n",
      "[1999]\ttrain-mlogloss:0.44719\ttest-mlogloss:0.65998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hdy25\\anaconda3\\lib\\site-packages\\xgboost\\core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "tfv = tfidf()\n",
    "tfv.fit(df_re.new_text.values.tolist())\n",
    "xtrain_tfv =  tfv.transform(x_train.new_text.values.tolist()) \n",
    "xtest_tfv = tfv.transform(x_test.new_text.values.tolist())\n",
    "\n",
    "pred_test_y, model = runXGB(xtrain_tfv, y_train['target'],\n",
    "                            xtest_tfv, y_test['target'], seed_val=0, child=1, colsample=0.3, verbose_eval=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13705, 14604) (5874, 14604)\n",
      "[16:00:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-mlogloss:1.08521\ttest-mlogloss:1.08637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hdy25\\anaconda3\\lib\\site-packages\\xgboost\\core.py:525: FutureWarning: Pass `evals` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\ttrain-mlogloss:0.60832\ttest-mlogloss:0.77876\n",
      "[600]\ttrain-mlogloss:0.49041\ttest-mlogloss:0.75930\n",
      "[900]\ttrain-mlogloss:0.40461\ttest-mlogloss:0.75322\n",
      "[935]\ttrain-mlogloss:0.39610\ttest-mlogloss:0.75312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hdy25\\anaconda3\\lib\\site-packages\\xgboost\\core.py:90: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "tfv = tfidf()\n",
    "tfv.fit(df_re.new_text.values)\n",
    "\n",
    "xtrain_tfv =  tfv.transform(x_train.new_text.values) \n",
    "xvalid_tfv = tfv.transform(x_test.new_text.values)\n",
    "print(xtrain_tfv.toarray().shape, xvalid_tfv.toarray().shape)\n",
    "\n",
    "svd = TruncatedSVD(n_components=120)\n",
    "svd.fit(xtrain_tfv)\n",
    "xtrain_svd = svd.transform(xtrain_tfv)\n",
    "xvalid_svd = svd.transform(xvalid_tfv)\n",
    "\n",
    "# # Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "# scl = StandardScaler()\n",
    "# scl.fit(xtrain_svd)\n",
    "# xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "# xvalid_svd_scl = scl.transform(xvalid_svd)\n",
    "\n",
    "pred_test_y, model = runXGB(xtrain_svd, y_train['target'],\n",
    "                            xvalid_svd, y_test['target'], seed_val=0, child=1, colsample=0.3, verbose_eval=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Word2Vec"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8aaa48b811dc8b83dd9153acc5544966249a7940997abc01b8de0ecd51e32347"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "간단한 모델로의 예시\n",
    "world_size는 사용가능한 GPU의 갯수를 의미한다.\n",
    "\"\"\"\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '8892'\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def example(rank, world_size):\n",
    "  #default process group 을 만듬\n",
    "  dist.init_process_group(\"gloo\", rank = rank, world_size = world_size)\n",
    "  \n",
    "  #간단한 모델\n",
    "  model = nn.Linear(10, 10)\n",
    "  \n",
    "  #DDP model 만들기\n",
    "  ddp_model = DDP(model)\n",
    "  \n",
    "  loss_fn = nn.MSELoss()\n",
    "  optimizer = optim.SGD(ddp_model.parameters(), lr = 0.001)\n",
    "  \n",
    "  #순전파\n",
    "  \n",
    "  for i in range(1500):\n",
    "\n",
    "    outputs = ddp_model(torch.randn(20, 10).to(device))\n",
    "    labels = torch.randn(20, 10).to(device)\n",
    "  \n",
    "    #역전파\n",
    "    loss_fn(outputs, labels).backward()\n",
    "  \n",
    "    #parameter 업데이트\n",
    "    optimizer.step()\n",
    "    print('one step')\n",
    "def main():\n",
    "  world_size = 2\n",
    "  mp.spawn(example, args = (world_size, ), nprocs = world_size, join = True)\n",
    "  \n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

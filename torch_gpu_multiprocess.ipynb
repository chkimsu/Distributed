{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 사실상 python multiprocessing 에서 조금 추가한게 torch.multiprocessing 이다. 큰 차이는 없다. 공식설명 : \n",
    "\n",
    ">torch.multiprocessing is a drop in replacement for Python’s multiprocessing module. It supports the exact same operations, but extends it, so that all tensors sent through a multiprocessing.Queue, will have their data moved into shared memory and will only send a handle to another process.\n",
    "\n",
    "> process간 자원공유는 이루어지지 않는다. multiprocessing에서는... 그렇기에 torch.distributed 를 통해서 process간 통신이 이루어지게 한다. torch.distributed는 멀티프로세스에 대해서, 멀티프로세스간 통신 및 취합이 가능하게 해준다. \n",
    "\n",
    "> 그렇기에 torch.nn.parallel.ddp 처럼 멀티프로세스로 돌아가는 것의 경우에는 멀티프로세스 생성하고 프로세스간 통신을 하고 gradient을 all-reduce 방식으로 취합할 수 있도록 해주는것이다. init_process_group을 통해 process간 통신을 master에서 관리하도록 하는것. \n",
    "\n",
    "> ddp의 경우는 원래는 mulit 머신, 즉 여러개의 컴퓨터 통신때문에 고안된것. dp는 이게 불가능하다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irteam/.local/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: initialization error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/irteam/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/irteam/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"/tmp/ipykernel_104360/2687867162.py\", line 9, in X_power_func\n    X_power = X**j\n  File \"/home/irteam/.local/lib/python3.7/site-packages/torch/_tensor.py\", line 31, in wrapped\n    return f(*args, **kwargs)\nRuntimeError: CUDA error: initialization error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_104360/2687867162.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Paralleizing over 2 GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_power_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: initialization error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.multiprocessing import Pool\n",
    "\n",
    "X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]])\n",
    "X = torch.DoubleTensor(X).cuda()\n",
    "\n",
    "def X_power_func(j):\n",
    "    X_power = X**j\n",
    "    return X_power\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  with Pool(processes = 2) as p:   # Paralleizing over 2 GPUs\n",
    "    results = p.map(X_power_func, range(8))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "간단한 모델로의 예시\n",
    "world_size는 사용가능한 GPU의 갯수를 의미한다.\n",
    "\"\"\"\n",
    "os.environ['MASTER_ADDR'] = '0.0.0.0'\n",
    "os.environ['MASTER_PORT'] = '8892'\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "def example(rank, world_size):\n",
    "  #default process group 을 만듬\n",
    "  dist.init_process_group(\"nccl\", rank = rank, world_size = world_size)\n",
    "  \n",
    "  #간단한 모델\n",
    "  model = nn.Linear(10, 10)\n",
    "  \n",
    "  #DDP model 만들기\n",
    "  ddp_model = DDP(model)\n",
    "  \n",
    "  loss_fn = nn.MSELoss()\n",
    "  optimizer = optim.SGD(ddp_model.parameters(), lr = 0.001)\n",
    "  \n",
    "  #순전파\n",
    "  \n",
    "  for i in range(1500):\n",
    "\n",
    "    outputs = ddp_model(torch.randn(20, 10).to(device))\n",
    "    labels = torch.randn(20, 10).to(device)\n",
    "  \n",
    "    #역전파\n",
    "    loss_fn(outputs, labels).backward()\n",
    "  \n",
    "    #parameter 업데이트\n",
    "    optimizer.step()\n",
    "    print('one step')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.multiprocessing.spawn.ProcessContext at 0x7f378578ca50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/irteam/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n",
      "    exitcode = _main(fd)\n",
      "  File \"/home/irteam/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'example' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/irteam/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/spawn.py\", line 105, in spawn_main\n",
      "    exitcode = _main(fd)\n",
      "  File \"/home/irteam/.pyenv/versions/3.7.5/lib/python3.7/multiprocessing/spawn.py\", line 115, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'example' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "world_size = 2\n",
    "mp.spawn(example, args = (world_size, ), nprocs = world_size, join = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94b38434f8b781c7ec7e7a220184b51170ceb1c75a14f96c983680aa99d1fed3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('sbchoi-workspace')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
